{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Willi Wu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from global_methods import *\n",
    "from global_constants import *\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import threading\n",
    "import seaborn as sb\n",
    "from fastai.tabular.core import df_shrink\n",
    "import datetime\n",
    "import win32file\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from itertools import repeat\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "win32file._setmaxstdio(6000)\n",
    "def join_all_data(name, filepath, lock):\n",
    "    output = open(name,'a', encoding=\"utf-8\")\n",
    "    raw_df = pd.read_csv(filepath, on_bad_lines=\"skip\",encoding='utf-8')\n",
    "    raw_df = raw_df.to_string(header=False,index=False)\n",
    "    with lock:\n",
    "        output.write(raw_df)\n",
    "        raw_df = clean(raw_df)\n",
    "        output.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_list = create_file_list(SPEND_PATTERNS, '.csv')\n",
    "output_dir = \"D:\\\\Code\\\\Safegraph_Project\\\\Testing\\\\Data\\\\initial_analysis.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = True\n",
    "# runtime: 5 min\n",
    "if run:\n",
    "    lock = threading.Lock()\n",
    "    threads = [threading.Thread(target=join_all_data,args=(output_dir, file, lock)) for file in raw_csv_list]\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [ 'placekey',\n",
    "            'location_name',\n",
    "            'naics_code',\n",
    "            'street_address',\n",
    "            'city',\n",
    "            'region',\n",
    "            'tracking_closed_since',\n",
    "            'spend_date_range_start', \n",
    "            'spend_date_range_end',\n",
    "            'raw_total_spend',\n",
    "            'raw_num_transactions',\n",
    "            'raw_num_customers']\n",
    "names = [   'placekey',\n",
    "            'parent_placekey'\t\n",
    "            'location_name'\t\n",
    "            'safegraph_brand_ids',\t\n",
    "            'brands',\t\n",
    "            'top_category',\t\n",
    "            'sub_category',\t\n",
    "            'naics_code',\t\n",
    "            'latitude',\t\n",
    "            'longitude',\t\n",
    "            'street_address',\t\n",
    "            'city',\t\n",
    "            'region',\t\n",
    "            'postal_code',\t\n",
    "            'iso_country_code',\n",
    "            'phone_number',\n",
    "            'open_hours',\n",
    "            'category_tags',\n",
    "            'opened_on',\n",
    "            'closed_on',\n",
    "            'tracking_closed_since',\n",
    "            'geometry_type',\n",
    "            'spend_date_range_start',\n",
    "            'spend_date_range_end',\n",
    "            'raw_total_spend',\n",
    "            'raw_num_transactions',\t\n",
    "            'raw_num_customers',\n",
    "            'median_spend_per_transaction',\n",
    "            'median_spend_per_customer',\n",
    "            'spend_per_transaction_percentiles',\n",
    "            'spend_by_day',\n",
    "            'spend_per_transaction_by_day',\n",
    "            'spend_by_day_of_week',\n",
    "            'day_counts',\n",
    "            'spend_pct_change_vs_prev_month',\n",
    "            'spend_pct_change_vs_prev_year',\n",
    "            'online_transactions',\n",
    "            'online_spend',\t\n",
    "            'transaction_intermediary',\n",
    "            'spend_by_transaction_intermediary',\n",
    "            'bucketed_customer_frequency',\t\n",
    "            'mean_spend_per_customer_by_frequency',\t\n",
    "            'bucketed_customer_incomes',\t\n",
    "            'mean_spend_per_customer_by_income',\t\n",
    "            'customer_home_city']\n",
    "\n",
    "# Some Lines contain unique characters that cannot be encoded in utf-8. Like the copyright symbol\n",
    "limited_view_df = pd.read_csv(output_dir, names=names, usecols=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max.columns',None)\n",
    "pd.set_option('display.width',1000)\n",
    "pd.set_option('display.colheader_justify','center')\n",
    "pd.set_option('display.precision',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: How unique are the placekeys? Are there multiple entries every month? Is there consistnecy across the years?\n",
    "# 1. Spend_date_range_start and spend_date_range_end are going to be cleaned up. I only want YYYY-MM-DD\n",
    "# 2. Create a new unique identifier -> merging placekey and the spend_date\n",
    "\n",
    "# 3. If when I filter based off of this new id and I get a duplicate entry -> Indicates multiple entries every month\n",
    "# 4. Expectation is that I see a single completely unique placekey for every month up until the business closes or temporarily closes\n",
    "\n",
    "data_df = limited_view_df.copy()\n",
    "# 762,721 unique placekeys out of 12 million records \n",
    "unique_placekeys = data_df['placekey'].unique()\n",
    "\n",
    "time_split = ['spend_date_range_start', 'spend_date_range_end']\n",
    "for col in time_split:\n",
    "    data_df[col] = data_df[col].apply(lambda x: x.split('T')[0])\n",
    "    data_df[col] = data_df[col].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\"))\n",
    "    \n",
    "data_df['new_placekey'] = str(data_df['placekey'][0]) + '-' + str(data_df['spend_date_range_start'])\n",
    "first_column = data_df.pop('new_placekey')\n",
    "data_df.insert(0, 'new_placekey', first_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for placekey in unique_placekeys[0:1]:\n",
    "    placekey_df = data_df[(data_df['placekey']==placekey)]\n",
    "    display(placekey_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_view_df['spend_date_range_start'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['spend_date_range_start'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weird = ['postal_code', 'tracking_closed_since', 'open_hours', '-149.869377', 'sub_category', ' \"\"18:00\"\"]]']\n",
    "# for index,row in data_df.iterrows():\n",
    "#     value = row['spend_date_range_start']\n",
    "#     if value in weird:\n",
    "#         print(index, row['place_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_processing import *\n",
    "test_list = create_file_list(r'D:\\Code\\Safegraph_Project\\SG_Data\\spend_patterns\\y=2021\\m=3', '.csv')\n",
    "total_test_df = pd.DataFrame()\n",
    "for file in test_list:\n",
    "    name_split = file_name_preprocessing(file)\n",
    "    try:\n",
    "        raw_df = pd.read_csv(file, on_bad_lines=\"skip\")\n",
    "    except:\n",
    "        pass\n",
    "    raw_df['source_location-year'] = name_split[0]\n",
    "    raw_df['source_location-month'] = name_split[1]\n",
    "    raw_df['source_location-part'] = name_split[2]\n",
    "    total_test_df = pd.concat([total_test_df,raw_df])        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85fec92199cc14a2bd4b3d9b9709b8077b8d4ba9daddeb167bcfeac63be00291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
